\documentclass{article}
\usepackage{a4wide}
\usepackage{cite}



%% if your are not using LaTeX2e use instead
%% \documentstyle[bnaic]{article}

%% begin document with title, author and affiliations

\title{Seminarium TI 3700\\ Data Analysis on Twitter: An Overview (working title)}
\author{A. Hambenne  \and
    O. Maas \and
    R. Oosterloo}
\date{}

\pagestyle{empty}

\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
\end{abstract}


\section{Introduction}

\newpage

\section{Data}

\subsection{Twitter API}

When Twitter was conceived, the developers decided that to adopt an API (Application Specific Interface) for third party developers. This has proven to be very
beneficial to both Twitter's popularity and alot of its users, including researchers.

An overview of the API structure and some useful fields:
\begin{enumerate}
\item \textbf{Tweets} The short messages sent by users
	\subitem \textbf{Coordinates} Retrieves the longitude and latitude position from where a specified tweet has been sent
	\subitem \textbf{Entities} Retrieves the entities (url, hashtag, etc.) that are embedded into the tweet
	\subitem \textbf{Lang} Retrieves the language in which a tweet has been sent
	\subitem \textbf{User} Retrieves the user that sent a tweet
\item \textbf{Users} The accounts that send tweets
	\subitem \textbf{Description} Retrieves the description that the user defined for his account
	\subitem \textbf{Location} Retrieves the location that the user defined for his account
	\subitem \textbf{Status} Retrieves, if possible, the user's latest tweet
\item \textbf{Entities} Images, hashtags and other entities that tweets can hold
	\subitem \textbf{Hashtags} Retrieves the hashtags that have been parset out of a tweet
	\subitem \textbf{Media} Retrieves the media that is associated with a tweet
	\subitem \textbf{urls} Retrieves the urls embedded in a tweet
	\subitem \textbf{user\_mentions} Retrieves the users that have been mentioned in a tweet
\item \textbf{Places} Predefined places on Twitter that users can associate with
	\subitem \textbf{Country} Retrieves the country associated with a place
	\subitem \textbf{Full\_name} Retrieves the full name of a place, for example Kanses ``City, Kansas''
	\subitem \textbf{Place\_type} Retrieves the type of a place, for example ``city''
\end{enumerate}

When the above fields are combined, it is possible to create sets of Tweets that fulfill a certain specification. These sets can be used to extract information.
No matter how specific you can define your search, sometimes you need a random sample of tweets to analyse. For this purpose, Twitter has a stream that
third party developers can tap into to retrieve random tweets. Unfortunately, the Twitter stream can only retrieve tweets that are less than a few days old.

The following of this article describes algorithms that dive into the big data of Twitter, how to extract useful information from the tweets, and what the information
can be applied to.

\newpage

\section{Single tweet analysis}
In this section we will discuss the different types of analysis that can be done on one single tweet. This usually is also the basis for a analysis of a larger amount of tweets or even on a stream of tweets. 
\subsection{Data enrichment}
At the very basis of a tweet is data enrichment. Because the length of a tweet is restricted to 140 characters the users of Twitter use language in creative ways that sometimes make it, even for humans, hard to understand what a tweet is about. To get a better understanding of the meaning of a tweet, a context is needed. For humans this is done by linking the information from the tweet to earlier acquired knowledge.
\subsubsection{Data enrichment issues}
The problem of linking tweets to another source of information poses the biggest issue in data enrichment. The tweets themselves do not provide much information. Fortunately, Kwak et al. have shown that the topic of over 85\% of the of the tweet is related to news \cite{newsmedia}. This knowledge can be leveraged to create a context for most of the tweets.  This is what what Abel et al. proposed to do\cite{enrichmentForProfiling}. They linked tweets to mainstream news networks like CNN, BBC and New York Times.

Linking to just news articles narrows the problem significantly, however, actually linking the tweets and the new articles still is an issue. Abel et al. propose two main linking strategies, divided into five methods total.
\begin{enumerate}
\item \textbf{URL-based strategy}\\
This strategy is based upon the observation that many tweets contains URLs. Usually these URLs refer to news articles the tweets are closely related to.
	\subitem \textbf{Strict URL-based} defines an article and a tweet as related when the tweet contains an URL to that news article.
	\subitem \textbf{Lenient URL-based}, this method is an extension of the strict URL-based method. This extension entails that if a tweet $t$ has a link to an article $a$, all tweets that are retweeting $t$ considered linked to $a$. The same counts for tweets that are in responds to $t$, they will also be linked to $n$.
\item \textbf{Content-based strategy}
	\subitem \textbf{Bag of words}, in this method a tweet is represented as a vector of word frequencies. For a news article we can do the same thing. To link an article to a tweet we need to calculate the similarity between the tweet and the article and pick the one with the highest similarity. The similarity is calculated by the $TF \times IDF$ (Term Frequency times Inverse Document Frequency): the number of times a word from the tweet occurs in a news article, multiplied by the logarithm of the total number of articles over the number of articles the word occurs in. 
	\subitem \textbf{Hashtag-based} is based upon the same concept as the Bag of words method. The difference here is that we do not consider the words in the article but we are only interested in the hashtags in the tweet for our representation of the tweet.
	\subitem \textbf{Entity-based} is also based upon the Bag of words method. The entity-based method does not consider words in an article. It relies on the entities that are mentioned in the article to make a vector to represent the article. 
\end{enumerate}
Using these method a broader context can be created. This still is not much more useful than the tweet on its own. For the article to be actually useful the topics of the articles have to be extracted. To do this Natural Language Processing and machine learning can be used to extract the entities from the article. OpenCalais\footnote{www.opencalais.com} is a web service that does just that. It returns not only the entities, it will also return facts and events hidden in the text, which will make the result much more useful.

\subsection{Sentiment analysis}
Sentiment analysis or opinion mining is "a linguistic analysis technique where a body of text is examined to characterize the tonality of the document."\footnote{http://lexicon.ft.com/Term?term=sentiment-analysis} In other words, the goal of a sentiment analysis is to extract the writers opinion on some topic from a body of text. This body of text can very well be a tweet, and that combined with the popularity of microblogging amongst internet users, gives a very interesting body of knowledge. But in order to get to that knowledge the data has to be analyzed fist. 
The most important and most obvious issue is how to extract an opinion from a tweet. In the next part we will discuss a few ways to extract an opinion from a tweet.
\subsubsection{Semantics }
The most important way to extract a sentiment from a tweet is by analyzing the semantics. After all, tweets are written text. The tweets can be labeled using machine learning algorithms. Pang et al. \cite{machineLearning} have done a comparative study of three standard machine learning algorithms: 
\begin{enumerate}
\item \textbf{Naive Bayes} is a fair simple classifier that operates under the assumption that the presents or absence of features does not relate to the presents of other features. If we apply that on Twitter we would get $c = \arg\max_{c\in C} P(c|w)$ where $c$ is a sentiment class, $C$ the collection of sentiment classes and $w$ is a tweet. \cite{sentAnalysis}
\item \textbf{Maximum Entropy} is a model that, unlike Bayes, does not make an independence assumption. It operates under the philosophy that "we should choose the model making the fewest assumptions about the data while still remaining consistent with it".\cite{machineLearning} \\
The formula is of the form $P(c|d) = \frac{1}{Z(d)}\exp(\Sigma_i \lambda_{i,c}F_{i,c}(d,c))$, where $Z(d)$ is a normalization function, $\lambda_{i,c}$ a weighted vector determining the significance of the feature and $F_{i,c}$ is a class/feature function. 
\item \textbf{Support Vector Machine} is fundamentally different from Naive Bayes and Max Entropy because it is not a probabilistic model. The basic idea behind a support vector machine is that a vector is trained to separate the sentiments. Not only will it separate the training data, it will do so at the largest possible margin. 
\end{enumerate}
It turns out that over the whole the Support Vector Machine performs the best in the sentiment analysis\cite{machineLearning}. The differences, however, are not substantially large. 

\subsubsection{Emoticons}
Other than semantical analysis we can also use other data from tweets. A good sentiment indicator are emoticons. Twitter users use emoticons (e.g. ":-)", ":(" or "=(") to indicate their mood.\cite{twitterSentiment} Since the number of emoticons is limited and therefor a sentiment can be assigned easily, the information from the emoticons can be leveraged in two ways
\begin{enumerate}
\item \textbf{Labeling:} The emoticons can be used to assign a sentiment to a tweet. This can be a very useful asset to take into account when assigning sentiments. The emoticons can be considered unigrams for some other classifier. There they will be strong evidence for a certain sentiment.
\item \textbf{Training:} Because of their clear evidence for a sentiment emoticons can also be used to build a training corpus. Twitter can be queried for "positive" emoticons and "negative" emoticons and by assuming the emoticon reflects the sentiment of the tweet, it can be labeled accordingly.\cite{moodLens} 
\end{enumerate}


\subsection{Concept extraction}
\subsection{Metadata analysis (opt)}

\section{Tweet stream analysis}
\subsection{Statistics}
\subsection{User Modelling}
\subsection{User Classification}
User classification is the problem of classification of users based on (1) their metadata such as age, location and job and (2) the tweets a user has posted. In this chapter we will discuss some issues concerning User Classification and some use-cases. 

\subsubsection{User Classification Issues}
\textbf{Context}\\
To classify users on their tweets is it useful to extract the context of the content of a tweet. Some hashtag $x$ could have a different context in two tweets. So extra information about the content of a tweet is needed. For instance \cite{userclasst} uses mentions of (members of) political parties to deduce if a certain hashtag is used in a political context. 
\\\\
\textbf{Profile}\\
Profile information can be helpful to classify a user, but it should be used with care due to the amount of fake, inconsistent or incorrect information. According to \cite{usermachine} 80\% of the users provide a location and 48\% of the users provide a short bio. Furthermore etnicity and gender are far more difficult to extract. 

\subsection{Tag based analysis}

Twitter allows their users to contribute their own social annotations to their tweets in the form of tags. These tags can be arbitrarily generated by users 
which make them hold interesting information about both the tweet and the user that posted it.

\begin{enumerate}
\item \textbf{Linear-Weighted Hybrid}\\

%TODO: Technische stuff begrijpen zodat ik die hier neer kan zetten + bibliography laten doorverwijzen

\item \textbf{Pair-Wise Interaction Tensor Factorization} The Pair-Wise Interaction Tensor Factorisation is a state-of-the-art algorithm that has
been developed for the purpose of tag-based recommendation systems. The model can be adapted to compute recommendations from tags used by Twitter.
	
\end{enumerate}


\subsection{Event identification}

Event identification is the problem of identifying events(e.g. concerts, festivals \& incidents) in a twitter-stream. The events identified range from Worldwide to local events. Twitter-messages related to an event can be a useful source of information as they often spread news earlier than the 'old' media (e.g News Papers \& News Sites)

To identify an event using an algorithm, a definition of an event is needed. \cite{eventident} defines an event as: "a real world occurence $e$ with (1) an associated Time Period $T_e$ and (2) a time-ordered stream of Twitter messages $M_e$, of substantial volume, discussing the
occurrence and published during time $T_e$."

\subsubsection{Event Identification Issues}
In this section we will discuss several issues regarding Event Identification and the solutions associated with them.
\\\\ 
\textbf{Separation of Event and Non-event Content}\\
Because not all tweets on twitter are event content, a seperation of Event and Non-event content is done prior of identifying events. To achieve this separation, \cite{eventident} uses an incremental clustering algorithm(ICA). ICA doesn't require an number of clusters before the execution of the algorithm. This is an useful feature in the constantly changing twitter-environment.

After the clustering a number of features is used to distinguish between event and non-event clusters. The following features are used by \cite{eventident} to identify an event cluster. 
\begin{itemize}
  \item Temporal: The characteristics of the volume over the time $T_e$ associated with an event. 
  \item Social: The interactions between users(e.g. retweets, replies \& mentions) during events could differ from other activities. 
  \item Topical: Tweets about events tend to center more around one central topic. Furthermore messages in event clusters are more likely to share one or more event-specific key-terms.  
  \item Twitter-centric: Some activities behave like an event but are Twitter-centric.(e.g. Tweets like 'Goodmorning!') These activities need to be filtered. 
\end{itemize}

With this list of features two event-classifiers are trained using two different techniques. The first classifier is trained using the "WEKA tookit"\footnote{http://www.cs.waikato.ac.nz/ml/weka/}, in the second classifier the  Naive-Bayes technique is used. According to \cite{eventident} the first classifier outperforms the second classifier in the separation of event and non-event clusters. 
\\\\
\textbf{Aligning Tweets with an Event} \\
On average thousands of tweets are sent every minute using Twitter(bron) . One of the main challenges in Event Identification is the alignment of tweets with their respective event. \cite{eventalign} tries to create an automated mapping solution for this problem using semantics.

At first the data is enriched, Twitter exposes the data using different formats such as JSON \& XML. To make this data more machine readable \cite{eventalign} enrich the data with sematics, after this process the data can then  be used as linked data which is more machine readable. The full enrichment process is described in section 2 of \cite{eventalign}.

%TODO misschien wat meer info over het semantic web toevoegen.
After this process \cite{eventalign} uses multiple techniques to align a tweet with the respective event. The first technique used by \cite{eventalign} is Feature Extraction. Three featuresets are used:
\begin{itemize}
%TODO uitwerken
  \item Immediate Resource Leaves: TODO
  \item 1-Step Resource Leaves 
  \item DBPedia Concepts 
\end{itemize}
%TODO afmaken

\subsubsection{Usecase: Earthquake Detection}
In \cite{earthq} a usecase for Event Identification is presented in the form of a earthquake detection system. \cite{earthq} considers twitter users as sensors. If enough sensors are activated (Tweets sent by a user are classified as a tweet about an earthquake), it is likely that an earthquake is occurring. 

To distinguish between tweets about Earthquakes a number of features is used. (1) The number of words in a tweet (2) The words in a tweet (3) Word context features. Feature (1) is a surprisingly good way to classify tweets about earthquakes, especially if it is used with a big amount of users. 

With the above feature set as basis, a system that detects earthquakes using twitter was built. After the system detects an earthquake an email to all users is sent. The system was used during 18. august - 2 September. In this time period 10 earthquakes had occurred, all of these earthquakes were detected. Furthermore \cite{earthq}'s system detects an earthquake 10 minutes after happening and sends a message within a minute after detection. 

\newpage

\section{Conclusion}

\newpage

\bibliographystyle{plain}
\bibliography{bibdb}


\end{document}








