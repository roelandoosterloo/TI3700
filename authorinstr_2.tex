\documentclass{article}
\usepackage{a4wide}
\usepackage{cite}



%% if your are not using LaTeX2e use instead
%% \documentstyle[bnaic]{article}

%% begin document with title, author and affiliations

\title{Seminarium TI 3700\\ Data Analysis on Twitter: An Overview (working title)}
\author{A. Hambenne  \and
    O. Maas \and
    R. Oosterloo}
\date{}

\pagestyle{empty}

\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
This is the abstract of my paper.
\end{abstract}


\section{Introduction}


\section{Data}

\subsection{Twitter API}
\section{Single tweet analysis}
In this section we will discuss the different types of analysis that can be done on one single tweet. This usually is also the basis for a analysis of a larger amount of tweets or even on a stream of tweets. 
\subsection{Data enrichment}
At the very basis of a tweet is data enrichment. Because the length of a tweet is restricted to 140 characters the users of Twitter use language in creative ways that sometimes make it, even for humans, hard to understand what a tweet is about. To get a better understanding of the meaning of a tweet, a context is needed. For humans this is done by linking the information from the tweet to earlier acquired knowledge.
\subsubsection{Data enrichment issues}
The problem of linking tweets to another source of information poses the biggest issue in data enrichment. The tweets themselves do not provide much information. Fortunately, Kwak et al. have shown that the topic of over 85\% of the of the tweet is related to news \cite{newsmedia}. This knowledge can be leveraged to create a context for most of the tweets.  This is what what Abel et al. proposed to do\cite{enrichmentForProfiling}. They linked tweets to mainstream news networks like CNN, BBC and New York Times. \\
Linking to just news articles narrows the problem significantly, however, actually linking the tweets and the new articles still is an issue. Abel et al. propose two main linking strategies, divided into five methods total.
\begin{enumerate}
\item \textbf{URL-based strategy}\\
This strategy is based upon the observation that many tweets contains URLs. Usually these URLs refer to news articles the tweets are closely related to.
	\subitem \textbf{Strict URL-based} defines an article and a tweet as related when the tweet contains an URL to that news article.
	\subitem \textbf{Lenient URL-based}, this method is an extension of the strict URL-based method. This extension entails that if a tweet $t$ has a link to an article $a$, all tweets that are retweeting $t$ considered linked to $a$. The same counts for tweets that are in responds to $t$, they will also be linked to $n$.
\item \textbf{Content-based strategy}
	\subitem \textbf{Bag of words}, in this method a tweet is represented as a vector of word frequencies. For a news article we can do the same thing. To link an article to a tweet we need to calculate the similarity between the tweet and the article and pick the one with the highest similarity. The similarity is calculated by the $TF \times IDF$ (Term Frequency times Inverse Document Frequency): the number of times a word from the tweet occurs in a news article, multiplied by the logarithm of the total number of articles over the number of articles the word occurs in. 
	\subitem \textbf{Hashtag-based} is based upon the same concept as the Bag of words method. The difference here is that we do not consider the words in the article but we are only interested in the hashtags in the tweet for our representation of the tweet.
	\subitem \textbf{Entity-based} is also based upon the Bag of words method. The entity-based method does not consider words in an article. It relies on the entities that are mentioned in the article to make a vector to represent the article. 
\end{enumerate}
Using these method a broader context can be created. This still is not much more useful than the tweet on its own. For the article to be actually useful the topics of the articles have to be extracted. To do this Natural Language Processing and machine learning can be used to extract the entities from the article. OpenCalais\footnote{www.opencalais.com} is a web service that does just that. It returns not only the entities, it will also return facts and events hidden in the text, which will make the result much more useful.
\subsection{Sentiment analysis}
\subsection{Concept extraction}
\subsection{Metadata analysis (opt)}

\section{Tweet stream analysis}
\subsection{Statistics}
\subsection{User Modelling}
\subsection{User Classification}


\subsection{Tag based analysis}
\subsection{Event identification}

Event identification is the problem of identifying events(e.g. concerts, festivals \& incidents) in a twitter-stream. The events identified range from Worldwide to local events. Twitter-messages related to an event can be a useful source of information as they often spread news earlier than the 'old' media (e.g News Papers \& News Sites)

To identify an event using an algorithm, a definition of an event is needed. \cite{eventident} defines an event as: "a real world occurence $e$ with (1) an associated Time Period $T_e$ and (2) a time-ordered stream of Twitter messages $M_e$, of substantial volume, discussing the
occurrence and published during time $T_e$."

\subsubsection{Event Identification Issues}
In this section we will discuss several issues regarding Event Identification and the solutions associated with them.
\\\\ 
\textbf{Separation of Event and Non-event Content}\\
Because not all tweets on twitter are event content, a seperation of Event and Non-event content is done prior of identifying events. To achieve this separation, \cite{eventident} uses an incremental clustering algorithm(ICA). ICA doesn't require an number of clusters before the execution of the algorithm. This is an useful feature in the constantly changing twitter-environment.

After the clustering a number of features is used to distinguish between event and non-event clusters. The following features are used by \cite{eventident} to identify an event cluster. 
\begin{itemize}
	%TODO Temporal en Twittercentric uitwerken.
  \item Temporal:
  \item Social: The interactions between users(e.g. retweets, replies \& mentions) during events could differ from other activities. 
  \item Topical: Tweets about events tend to center more around one central topic. Furthermore messages in event clusters are more likely to share one or more event-specific key-terms.  
  \item Twitter-centric: 
\end{itemize}
%TODO Weka toolkit uitzoeken
With this list of features two event-classifiers are trained using two different techniques. The first classifier is trained using the "WEKA tookit", in the second classifier the  Naive-Bayes technique is used. According to \cite{eventident} the first classifier outperforms the second classifier in the separation of event and non-event clusters. 
\\\\
\textbf{Aligning Tweets with an Event} \\
On average thousands of tweets are sent every minute using Twitter(bron) . One of the main challenges in Event Identification is the alignment of tweets with their respective event. \cite{eventalign} tries to create an automated mapping solution for this problem using semantics.

At first the data is enriched, Twitter exposes the data using different formats such as JSON \& XML. To make this data more machine readable \cite{eventalign} enrich the data with sematics, after this process the data can then  be used as linked data which is more machine readable. The full enrichment process is described in section 2 of \cite{eventalign}.

%TODO misschien wat meer info over het semantic web toevoegen.
After this process \cite{eventalign} uses multiple techniques to align a tweet with the respective event. The first technique used by \cite{eventalign} is Feature Extraction. Three featuresets are used:
\begin{itemize}
%TODO uitwerken
  \item Immediate Resource Leaves: TODO
  \item 1-Step Resource Leaves 
  \item DBPedia Concepts 
\end{itemize}
%TODO afmaken

\subsubsection{Usecase: Earthquake Detection}
In \cite{earthq} a usecase for Event Identification is presented in the form of a earthquake detection system. \cite{earthq} considers twitter users as sensors. If enough sensors are activated (Tweets sent by a user are classified as a tweet about an earthquake), it is likely that an earthquake is occurring. 

To distinguish between tweets about Earthquakes a number of features is used. (1) The number of words in a tweet (2) The words in a tweet (3) Word context features. Feature (1) is a surprisingly good way to classify tweets about earthquakes, especially if it is used with a big amount of users. 

With the above feature set as basis, a system that detects earthquakes using twitter was built. After the system detects an earthquake an email to all users is sent. The system was used during 18. august - 2 September. In this time period 10 earthquakes had occurred, all of these earthquakes were detected. Furthermore \cite{earthq}'s system detects an earthquake 10 minutes after happening and sends a message within a minute after detection. 
\section{Conclusion}

\bibliographystyle{plain}
\bibliography{bibdb}


\end{document}








